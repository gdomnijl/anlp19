{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores methods for comparing two different textual datasets to identify the terms that are distinct to each one:\n",
    "\n",
    "* Difference of proportions (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) section 3.2.2\n",
    "* Mann-Whitney rank-sums test (described in [Kilgarriff 2001, Comparing Corpora](https://www.sketchengine.eu/wp-content/uploads/comparing_corpora_2001.pdf), section 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator\n",
    "from collections import Counter\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the convote data is already tokenized so just split on whitespace\n",
    "repub_tokens=open(\"../data/repub.convote.txt\", encoding=\"utf-8\").read().split(\" \")\n",
    "dem_tokens=open(\"../data/dem.convote.txt\", encoding=\"utf-8\").read().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: First, calculate the simple \"difference of proportions\" measure from Monroe et al.'s \"Fighting Words\", section 3.2.2.  What are the top ten terms in this measurement that are most republican and most democrat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove punctuation\n",
    "def difference_of_proportions(one_tokens, two_tokens):\n",
    "    # your code here\n",
    "    freq_dict1 = Counter(one_tokens)\n",
    "    freq_dict2 = Counter(two_tokens)\n",
    "    n1 = len(one_tokens)\n",
    "    n2 = len(two_tokens)\n",
    "    print(len(freq_dict1.keys()))\n",
    "    print(len(freq_dict2.keys()))\n",
    "\n",
    "    diff_dict = {}\n",
    "    \n",
    "    for w in set(freq_dict1.keys()).intersection(set(freq_dict2.keys())):\n",
    "        p1 = freq_dict1[w]/n1\n",
    "        p2 = freq_dict2[w]/n2\n",
    "        diff_prop = p1-p2\n",
    "        diff_dict[w] = diff_prop                                                \n",
    "    \n",
    "    ##if the word is not mentioned by the other corpus at all                                 \n",
    "    for w in set(freq_dict1.keys()).difference(set(freq_dict2.keys())): \n",
    "        p1 = freq_dict1[w]/n1\n",
    "        diff_prop = p1 - 0\n",
    "        diff_dict[w] = diff_prop\n",
    "    \n",
    "    for w in set(freq_dict2.keys()).difference(set(freq_dict1.keys())):\n",
    "        p2 = freq_dict2[w]/n2\n",
    "        diff_prop = 0 - p2\n",
    "        diff_dict[w] = diff_prop\n",
    "    return diff_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "13943\n"
     ]
    }
   ],
   "source": [
    "diff_dict = difference_of_proportions(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Republican words:\n",
      "('i', -0.002870948015418236)\n",
      "('we', -0.0020739540633471117)\n",
      "('and', -0.0017279456625680124)\n",
      "('of', -0.0014950519581076668)\n",
      "(',', -0.00105321588184943)\n",
      "('chairman', -0.0009598934247981516)\n",
      "('that', -0.000945583476245123)\n",
      "('as', -0.0009124972356492223)\n",
      "('gentleman', -0.0008093810284795912)\n",
      "('a', -0.0008020309007514565)\n",
      "****\n",
      "Most Democratic words:\n",
      "('not', 0.0015745433184340962)\n",
      "('$', 0.0015095648428079297)\n",
      "('cuts', 0.001031315818425968)\n",
      "('bill', 0.0010228370409021796)\n",
      "('republican', 0.001001288082839861)\n",
      "('budget', 0.0009261863664701928)\n",
      "('billion', 0.0008820967153998979)\n",
      "('would', 0.0007701123575280444)\n",
      "('health', 0.0007538336601492987)\n",
      "('for', 0.0007352277281844066)\n"
     ]
    }
   ],
   "source": [
    "most_repub = sorted(diff_dict.items(), key = lambda x: x[1])[:10]\n",
    "most_dem = sorted(diff_dict.items(), key = lambda x: x[1], reverse = True)[:10]\n",
    "\n",
    "print(\"Most Republican words:\")\n",
    "for r in most_repub:\n",
    "    print(r)\n",
    "print(\"****\\nMost Democratic words:\")    \n",
    "for d in most_dem:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply analyzing the difference in relative frequencies has a number of downsides: \n",
    "1.) As Monroe et al (2009) points out (and we can see here as well), it tends to emphasize high-frequency words (be sure you understand why). \n",
    "2.) We're not measuring whether a difference is statistically meaningful or just due to chance; the $\\chi^2$ test is one method (described in Kilgarriff 2001 and in the context of collocations in Manning and Schuetze [here](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)) that addresses the desideratum of finding statistically significant terms, but it too has another downside:\n",
    "3.) Simply counting up the total number of mentions of a term doesn't account for the \"burstiness\" of language -- if we see the word \"Dracula\" in a text, we're probably going to see it again in that same text.  The occurrence of words are not independent random events; they are tightly coupled with each other. If we're trying to understanding the robust differences between two corpora, we might prefer to prioritize words that show up more frequently *everywhere* in corpus A (but not in corpus B) over those that show up only very frequently within narrow slice of A (such as one text in a genre, one chapter in a book, or one speaker when measuring the differences between policital parties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 (check-plus): One measure that does account for this burstiness is the adaptation by corpus linguistics of the non-parametric Mann-Whitney rank-sum test. The specific adaptation of this test for text is described in Kilgarriff 2001, section 2.3.  Implement this test using a fixed chunk size of 500 and the [scikit-learn mannwhitneyu function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html); what are the top ten terms in this measurement that are most republican and most democrat? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(mannwhitneyu([1,2,3],[2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18434413468089078"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove punctuation\n",
    "\n",
    "## Citation: code adapted from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def divide_chunks(chunk_size, ls):\n",
    "    for i in range(0, len(ls), chunk_size):\n",
    "        yield ls[i:i + chunk_size]\n",
    "\n",
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    # your code here\n",
    "    \n",
    "    one_chunks = list(divide_chunks(500, one_tokens))\n",
    "    two_chunks = list(divide_chunks(500, two_tokens))\n",
    "    \n",
    "    freq_dict1 = Counter(one_tokens)\n",
    "    freq_dict2 = Counter(two_tokens)\n",
    "    \n",
    "    U_dict ={}\n",
    "\n",
    "    ## only using words that appear in both corpus\n",
    "    for word in set(freq_dict1.keys()).intersection(set(freq_dict2.keys())): \n",
    "        word_counts1 = []\n",
    "        word_counts2 = []\n",
    "        for c1 in one_chunks[:-1]: ## Leftover uneven chunks is discarded\n",
    "            wc = Counter(c1)\n",
    "            word_counts1.append(wc[word])\n",
    "        for c2 in two_chunks[:-1]:\n",
    "            wc = Counter(c2)\n",
    "            word_counts2.append(wc[word])\n",
    "        U_score = list(mannwhitneyu(word_counts1, word_counts2))\n",
    "        U_dict[word] = U_score\n",
    "    \n",
    "    return U_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict1 = Counter(dem_tokens)\n",
    "freq_dict2 = Counter(repub_tokens)\n",
    "one_chunks = list(divide_chunks(500, dem_tokens))\n",
    "two_chunks = list(divide_chunks(500, repub_tokens))\n",
    "word = next(enumerate(set(freq_dict1.keys()).intersection(set(freq_dict2.keys()))))[1]\n",
    "word_counts1 = []\n",
    "word_counts2 = []\n",
    "U_dict = {}\n",
    "for c1 in one_chunks[:-1]: ## Leftover uneven chunks is discarded\n",
    "    wc = Counter(c1)\n",
    "    word_counts1.append(wc[word])\n",
    "for c2 in two_chunks[:-1]:\n",
    "    wc = Counter(c2)\n",
    "    word_counts2.append(wc[word])\n",
    "U_score = list(mannwhitneyu(word_counts1, word_counts2))\n",
    "U_dict[word] = U_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [401544.5, 0.47111929808574066]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mann_whitney_analysis(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('republican', [301538.0, 1.1826838328124882e-40]),\n",
       " ('i', [325948.5, 1.5237584748910261e-12]),\n",
       " ('cuts', [327712.5, 3.27973742662632e-31]),\n",
       " ('$', [331406.5, 1.9349035907882228e-13]),\n",
       " ('not', [338030.5, 1.763811166528947e-09]),\n",
       " ('majority', [338080.5, 2.393937075482985e-18]),\n",
       " ('billion', [338275.0, 4.1701302576006746e-15]),\n",
       " ('republicans', [347467.0, 5.796945335930232e-19]),\n",
       " ('gentleman', [348427.5, 1.8181581506276446e-08]),\n",
       " ('important', [349640.5, 3.3402805110767186e-09]),\n",
       " ('administration', [349664.0, 2.6878084072105137e-13]),\n",
       " ('as', [350497.5, 7.934708861032621e-07]),\n",
       " ('--', [351024.0, 3.2654438775479844e-14]),\n",
       " ('?', [351656.5, 2.980457668426629e-08]),\n",
       " ('cut', [351719.5, 5.18595310935567e-16]),\n",
       " ('americans', [352241.0, 1.0568244786776509e-09]),\n",
       " ('support', [353646.5, 5.202330700143442e-07]),\n",
       " ('may', [353850.0, 1.515388968974397e-08]),\n",
       " ('chairman', [354063.5, 6.051956905100078e-07]),\n",
       " ('opposition', [354284.0, 3.977739558271767e-14]),\n",
       " ('thank', [354562.0, 2.8610681777752387e-08]),\n",
       " ('committee', [354965.0, 2.1202644155483443e-07]),\n",
       " ('no', [355484.5, 1.260097570471907e-06]),\n",
       " ('time', [355842.0, 4.273174882820142e-06]),\n",
       " ('health', [356624.5, 2.166626983086252e-08]),\n",
       " ('than', [356798.5, 1.6088611589648706e-06]),\n",
       " ('we', [357201.5, 1.869276460614514e-05]),\n",
       " ('instead', [357681.5, 2.4855694027891675e-13]),\n",
       " ('some', [357903.0, 2.3385539821801966e-06]),\n",
       " ('very', [358711.0, 1.5569654426710194e-06]),\n",
       " ('vote', [359667.0, 6.644136328323195e-07]),\n",
       " ('budget', [359712.0, 8.979343409134848e-08]),\n",
       " ('should', [359784.0, 6.113957031902095e-06]),\n",
       " ('and', [359860.5, 5.306108997159864e-05]),\n",
       " ('bill', [360082.5, 4.061296280391666e-05]),\n",
       " ('nothing', [360265.0, 2.260394027268939e-12]),\n",
       " ('act', [360295.0, 1.3266505886440864e-06]),\n",
       " ('of', [363088.5, 0.0001705075237803203]),\n",
       " ('my', [363568.5, 0.0001273143095547023]),\n",
       " (\"'s\", [364624.0, 0.00020271466515778554]),\n",
       " ('care', [364727.0, 4.3208723307763477e-07]),\n",
       " ('economy', [364763.5, 8.294754787491463e-09]),\n",
       " ('only', [365694.0, 4.523780680281529e-05]),\n",
       " ('(', [365729.0, 6.661904498381808e-05]),\n",
       " (')', [365767.0, 6.842913957500606e-05]),\n",
       " ('work', [365854.0, 2.7432785851528048e-05]),\n",
       " ('children', [365860.0, 5.812504481229053e-08]),\n",
       " ('oppose', [366033.0, 9.271770477455383e-12]),\n",
       " ('iraq', [366777.5, 3.678451395851637e-09]),\n",
       " ('be', [366957.0, 0.0005292722149959108]),\n",
       " ('would', [367005.5, 0.00046883936934565104]),\n",
       " ('with', [367412.0, 0.0005979639648621837]),\n",
       " ('growth', [368302.0, 8.553315261777002e-13]),\n",
       " ('his', [368962.5, 3.0341253706540012e-05]),\n",
       " ('from', [369310.0, 0.0010726273845893094]),\n",
       " ('``', [369333.5, 0.00015550027564550695]),\n",
       " ('want', [369404.5, 0.0003005580178849593]),\n",
       " ('will', [369514.5, 0.0011463910245602602]),\n",
       " ('workers', [369698.5, 1.4057611966929274e-06]),\n",
       " ('yield', [369737.5, 7.800949218795831e-06]),\n",
       " ('one', [369757.5, 0.0006738822416120097]),\n",
       " ('when', [369779.0, 0.0005439519305140591]),\n",
       " (\"''\", [369822.5, 0.00017160285929300313]),\n",
       " ('good', [370206.5, 3.860164847041858e-05]),\n",
       " ('mr.', [370341.5, 0.0015607909629593093]),\n",
       " ('debt', [370367.0, 9.697771946621523e-13]),\n",
       " ('people', [370567.0, 0.0009849564192605325]),\n",
       " ('forward', [370626.5, 1.5324213828198943e-07]),\n",
       " ('funding', [370735.0, 1.5749638640856197e-05]),\n",
       " ('issue', [370778.0, 2.65184412993171e-05]),\n",
       " ('even', [371127.5, 0.00013503316269983985]),\n",
       " ('look', [371280.0, 5.991006353783991e-06]),\n",
       " (\"n't\", [371337.5, 8.604458107699066e-09]),\n",
       " ('consume', [371443.5, 1.467504324482513e-07]),\n",
       " ('by', [371551.5, 0.0020675911029568537]),\n",
       " ('child', [371709.0, 1.6401204084010916e-08]),\n",
       " ('without', [371912.5, 2.3461564066889755e-05]),\n",
       " ('public', [372630.0, 5.029649316027729e-06]),\n",
       " ('2005', [372752.0, 1.988998337802908e-06]),\n",
       " ('democratic', [372972.0, 2.3456095822943874e-07]),\n",
       " ('myself', [373133.0, 2.7158262237523115e-05]),\n",
       " ('companies', [373285.5, 2.5444102811019722e-06]),\n",
       " ('does', [373385.5, 0.001069022918066688]),\n",
       " ('such', [373698.0, 0.0006799037683175724]),\n",
       " ('small', [373942.0, 2.0688973847375153e-05]),\n",
       " ('tax', [373991.5, 7.050651471123115e-05]),\n",
       " ('fact', [374029.0, 0.0003484674592910758]),\n",
       " ('jobs', [374059.5, 3.542648546663018e-06]),\n",
       " ('all', [374361.0, 0.003848044276499501]),\n",
       " ('protections', [374506.0, 7.186050039921427e-11]),\n",
       " ('pay', [374741.0, 2.8494470331189087e-05]),\n",
       " ('has', [374869.5, 0.004890962996255072]),\n",
       " ('a', [375356.0, 0.00681578854514438]),\n",
       " ('families', [375727.0, 0.00019915982299825523]),\n",
       " ('this', [375785.5, 0.007569599379195179]),\n",
       " ('bush', [375846.5, 1.7879247034793023e-06]),\n",
       " ('education', [376139.5, 3.625200131385617e-05]),\n",
       " ('fails', [376219.5, 2.117120327370357e-13]),\n",
       " ('appreciate', [376365.5, 8.290825985859536e-07]),\n",
       " ('united', [376366.0, 0.00180543488375834]),\n",
       " ('rights', [376514.0, 1.785277242450202e-05]),\n",
       " ('congress', [376649.5, 0.003696172921085375]),\n",
       " (',', [376763.0, 0.00986386246768297]),\n",
       " ('ensure', [376812.0, 2.7270309831793083e-05]),\n",
       " ('deficit', [376995.5, 2.15620678152095e-06]),\n",
       " ('programs', [377086.5, 0.00024921239154075155]),\n",
       " ('it', [377245.0, 0.010724818143433507]),\n",
       " ('businesses', [377424.5, 8.832690944969016e-06]),\n",
       " ('30', [377622.0, 4.54680329000014e-06]),\n",
       " ('reform', [378115.5, 6.753953261191138e-05]),\n",
       " ('business', [378245.0, 0.000301789666951075]),\n",
       " ('million', [378269.5, 0.0034495383852881653]),\n",
       " ('why', [378336.5, 0.0012253720298528673]),\n",
       " ('speaker', [378424.5, 0.011159137491339642]),\n",
       " ('yet', [378611.5, 0.0001793078644753885])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(d.items(), key = lambda x: x[1])[:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d,open(\"dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
